{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['data']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "c6aaea184c28cf12273c93a568123cdc25983d2c"
      },
      "cell_type": "markdown",
      "source": "### Ομοίως με πριν, φορτώνουμε τα δεδομένα μας σε Data Loaders."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import copy\nimport gzip\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import SubsetRandomSampler, DataLoader\n\nclass_mapping = {\n    'Rock': 'Rock',\n    'Psych-Rock': 'Rock',\n    'Indie-Rock': None,\n    'Post-Rock': 'Rock',\n    'Psych-Folk': 'Folk',\n    'Folk': 'Folk',\n    'Metal': 'Metal',\n    'Punk': 'Metal',\n    'Post-Punk': None,\n    'Trip-Hop': 'Trip-Hop',\n    'Pop': 'Pop',\n    'Electronic': 'Electronic',\n    'Hip-Hop': 'Hip-Hop',\n    'Classical': 'Classical',\n    'Blues': 'Blues',\n    'Chiptune': 'Electronic',\n    'Jazz': 'Jazz',\n    'Soundtrack': None,\n    'International': None,\n    'Old-Time': None\n}\n\n\ndef torch_train_val_split(\n        dataset, batch_train, batch_eval,\n        val_size=.2, shuffle=True, seed=17):\n    # Creating data indices for training and validation splits:\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    val_split = int(np.floor(val_size * dataset_size))\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(indices)\n    train_indices = indices[val_split:]\n    val_indices = indices[:val_split]\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    val_sampler = SubsetRandomSampler(val_indices)\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_train,\n                              sampler=train_sampler)\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_eval,\n                            sampler=val_sampler)\n    return train_loader, val_loader\n\n\ndef read_spectrogram(spectrogram_file, chroma=True):\n    with gzip.GzipFile(spectrogram_file, 'r') as f:\n        spectrograms = np.load(f)\n    # spectrograms contains a fused mel spectrogram and chromagram\n    # Decompose as follows\n    return spectrograms.T\n\n\nclass LabelTransformer(LabelEncoder):\n    def inverse(self, y):\n        try:\n            return super(LabelTransformer, self).inverse_transform(y)\n        except:\n            return super(LabelTransformer, self).inverse_transform([y])\n\n    def transform(self, y):\n        try:\n            return super(LabelTransformer, self).transform(y)\n        except:\n            return super(LabelTransformer, self).transform([y])\n\n        \nclass PaddingTransform(object):\n    def __init__(self, max_length, padding_value=0):\n        self.max_length = max_length\n        self.padding_value = padding_value\n\n    def __call__(self, s):\n        if len(s) == self.max_length:\n            return s\n\n        if len(s) > self.max_length:\n            return s[:self.max_length]\n\n        if len(s) < self.max_length:\n            s1 = copy.deepcopy(s)\n            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n            s1 = np.vstack((s1, pad))\n            return s1\n\n        \nclass SpectrogramDataset(Dataset):\n    def __init__(self, path, class_mapping=None, train=True, max_length=-1):\n        t = 'train' if train else 'test'\n        p = os.path.join(path, t)\n        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n        self.files, labels = self.get_files_labels(self.index, class_mapping)\n        self.feats = [read_spectrogram(os.path.join(p, f)) for f in self.files]\n        self.feat_dim = self.feats[0].shape[1]\n        self.lengths = [len(i) for i in self.feats]\n        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n        self.label_transformer = LabelTransformer()\n        if isinstance(labels, (list, tuple)):\n            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n\n    def get_files_labels(self, txt, class_mapping):\n        with open(txt, 'r') as fd:\n            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n        files, labels = [], []\n        for l in lines:\n            label = l[1]\n            if class_mapping:\n                label = class_mapping[l[1]]\n            if not label:\n                continue\n            files.append(l[0])\n            labels.append(label)\n        return files, labels\n\n    def __getitem__(self, item):\n        l = min(self.lengths[item], self.max_length)\n        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n\n    def __len__(self):\n        return len(self.labels)\n      \nif __name__ == '__main__':\n    specs = SpectrogramDataset('../input/data/data/fma_genre_spectrograms_beat', train=True, class_mapping=class_mapping, max_length=-1)\n    train_loader, val_loader = torch_train_val_split(specs, 32 ,32, val_size=.33)\n    test_loader = DataLoader(SpectrogramDataset('../input/data/data/fma_genre_spectrograms_beat', train=False, class_mapping=class_mapping, max_length=-1), 32)\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a007ac837b5c56cee7d36e89cd5ee0f478216a61"
      },
      "cell_type": "markdown",
      "source": "## **Ορισμός CNN model** \n### Αποτελείται από 4 layers για την εξαγωγή χαρακτηριστικών (2d Convolution, Batch Normalisation, ReLU activation function & 2d Max Pooling) και από ένα linear layer-dropout- linear layer για το classification."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9276f18a9202d125ae0565fdd104e43706b5b602"
      },
      "cell_type": "code",
      "source": "import torch.nn as nn\nimport torch\ntorch.set_default_tensor_type(torch.DoubleTensor)\n\nclass BasicCNN(nn.Module):\n    def __init__(self, input_dim, output_dim, channels=[12, 24, 48, 96], kernels=[3, 3, 3, 3]):\n        super(BasicCNN, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        \n        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=self.input_dim, out_channels=channels[0], kernel_size=kernels[0]),\n                                    nn.BatchNorm2d(channels[0]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[0]))\n        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=channels[0], out_channels=channels[1], kernel_size=kernels[1]),\n                                    nn.BatchNorm2d(channels[1]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[1]))\n        self.layer3 = nn.Sequential(nn.Conv2d(in_channels=channels[1], out_channels=channels[2], kernel_size=kernels[2]),\n                                    nn.BatchNorm2d(channels[2]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[2]))\n        self.layer4 = nn.Sequential(nn.Conv2d(in_channels=channels[2], out_channels=channels[3], kernel_size=kernels[3]),\n                                    nn.BatchNorm2d(channels[3]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[3]))\n        self.linear = nn.Sequential(nn.Linear(channels[3], channels[3]),\n                                    nn.Dropout(inplace=True),\n                                    nn.Linear(channels[3], self.output_dim))\n    \n    def forward(self, x):\n        x = x.view(x.shape[0], self.input_dim, x.shape[2], x.shape[1])\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(x.shape[0], -1)\n        x = self.linear(x)\n        \n        return x",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "93f5b275e25c4db2f1229d9fd4d9e8d06661edf9"
      },
      "cell_type": "markdown",
      "source": "## Εκπαίδευση και αξιολόγηση μοντέλου"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa56c84510f58e50781912530f419a6d2fa98708"
      },
      "cell_type": "code",
      "source": "def accuracy(model, data_loader):\n    model.eval()\n    correct, total = 0, 0\n    for feats, labels, lens in data_loader:\n        with torch.no_grad():\n            scores = model(feats)\n            prediction = scores.argmax(dim=1)\n            correct += (prediction == labels).sum().item()\n            total += feats.shape[0]\n    return 100 * correct / total\n\nmodel = BasicCNN(input_dim=1, output_dim=10)\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n\nfor epoch in range(20):\n    if epoch == 5:\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # fine tuning\n    model.train()\n    for feats, labels, lens in (train_loader):\n        # Step 1. Remember that Pytorch accumulates gradients.\n        # We need to clear them out before each instance\n        model.zero_grad()\n        optimizer.zero_grad()\n\n        # Step 3. Run our forward pass.\n        pred_labels = model(feats)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by\n        #  calling optimizer.step()\n        loss = loss_function(pred_labels, labels)\n        #print(\"Loss is: \" + str(loss))\n        loss.backward()\n        optimizer.step()\n    print(\"Epoch \" + str(epoch) + \" | Accuracy (on validation set) is: \" + str(accuracy(model, val_loader)))",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 0 | Accuracy (on validation set) is: 21.44736842105263\nEpoch 1 | Accuracy (on validation set) is: 29.736842105263158\nEpoch 2 | Accuracy (on validation set) is: 29.342105263157894\nEpoch 3 | Accuracy (on validation set) is: 37.63157894736842\nEpoch 4 | Accuracy (on validation set) is: 21.842105263157894\nEpoch 5 | Accuracy (on validation set) is: 38.421052631578945\nEpoch 6 | Accuracy (on validation set) is: 38.026315789473685\nEpoch 7 | Accuracy (on validation set) is: 38.55263157894737\nEpoch 8 | Accuracy (on validation set) is: 38.68421052631579\nEpoch 9 | Accuracy (on validation set) is: 40.526315789473685\nEpoch 10 | Accuracy (on validation set) is: 39.21052631578947\nEpoch 11 | Accuracy (on validation set) is: 38.1578947368421\nEpoch 12 | Accuracy (on validation set) is: 40.13157894736842\nEpoch 13 | Accuracy (on validation set) is: 39.60526315789474\nEpoch 14 | Accuracy (on validation set) is: 39.473684210526315\nEpoch 15 | Accuracy (on validation set) is: 38.421052631578945\nEpoch 16 | Accuracy (on validation set) is: 38.81578947368421\nEpoch 17 | Accuracy (on validation set) is: 40.39473684210526\nEpoch 18 | Accuracy (on validation set) is: 40.39473684210526\nEpoch 19 | Accuracy (on validation set) is: 39.86842105263158\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "a75a3e8038715e8273c00368090ffef68cb00cf7"
      },
      "cell_type": "markdown",
      "source": "### Ξεκινάμε αρχικά με σχετικά μεγάλο learning rate, αλλά μετά από ένα μικρό αριθμό εποχών το χαμηλώνουμε. Με 20 εποχές πετυχαίνουμε σημαντική βελτιώση στο validation set (σε σχέση με το LSTM, περί το 40%), ωστόσο, όπως βλέπουμε παρακάτω, τα αποτελέσματα στο training set είναι χειρότερα απ' τα αναμενόμενα."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89fe460e24873a199c5d0f39387cee4945c0f1c1"
      },
      "cell_type": "code",
      "source": "print(\"Accuracy on test set: \" + str(accuracy(model, test_loader)))",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Accuracy on test set: 14.08695652173913\n",
          "name": "stdout"
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}