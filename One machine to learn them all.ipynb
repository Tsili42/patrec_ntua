{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['data']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import copy\nimport gzip\nimport torch\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import SubsetRandomSampler, DataLoader\n\ndef torch_train_val_split(\n        dataset, batch_train, batch_eval,\n        val_size=.2, test_size=.3, shuffle=True, seed=42):\n    # Creating data indices for training and validation splits:\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    val_split = int(np.floor(val_size * dataset_size))\n    test_split = int(np.floor(test_size * val_split))\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(indices)\n    \n    train_indices = indices[val_split:]\n    val_indices = indices[:val_split]\n    test_indices = val_indices[:test_split]\n    val_indices = val_indices[test_split:]\n    \n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    val_sampler = SubsetRandomSampler(val_indices)\n    test_sampler = SubsetRandomSampler(test_indices)\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_train,\n                              sampler=train_sampler)\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_eval,\n                            sampler=val_sampler)\n    test_loader = DataLoader(dataset,\n                            batch_size=batch_eval,\n                            sampler=test_sampler)\n    \n    return train_loader, val_loader, test_loader\n\n\ndef read_spectrogram(spectrogram_file, chroma=True):\n    with gzip.GzipFile(spectrogram_file, 'r') as f:\n        spectrograms = np.load(f)\n    # spectrograms contains a fused mel spectrogram and chromagram\n    # Decompose as follows\n    return spectrograms.T\n\n\nclass LabelTransformer(LabelEncoder):\n    def inverse(self, y):\n        try:\n            return super(LabelTransformer, self).inverse_transform(y)\n        except:\n            return super(LabelTransformer, self).inverse_transform([y])\n\n    def transform(self, y):\n        try:\n            return super(LabelTransformer, self).transform(y)\n        except:\n            return super(LabelTransformer, self).transform([y])\n\n        \nclass PaddingTransform(object):\n    def __init__(self, max_length, padding_value=0):\n        self.max_length = max_length\n        self.padding_value = padding_value\n\n    def __call__(self, s):\n        if len(s) == self.max_length:\n            return s\n\n        if len(s) > self.max_length:\n            return s[:self.max_length]\n\n        if len(s) < self.max_length:\n            s1 = copy.deepcopy(s)\n            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n            s1 = np.vstack((s1, pad))\n            return s1\n\n        \nclass SpectrogramDataset(Dataset):\n    def __init__(self, path, class_mapping=None, train=True, max_length=-1):\n        t = 'train' if train else 'test'\n        p = os.path.join(path, t)\n        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n        self.files, self.labels = self.get_files_labels(self.index, class_mapping)\n        self.feats = [read_spectrogram(os.path.join(p, f)) for f in self.files]\n        self.feat_dim = self.feats[0].shape[1]\n        self.lengths = [len(i) for i in self.feats]\n        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n        self.label_transformer = LabelTransformer()\n        self.labels = np.array(self.labels)\n\n    def get_files_labels(self, txt, class_mapping):\n        with open(txt, 'r') as fd:\n            lines = [l.rstrip().split(',') for l in fd.readlines()[1:]]\n        files, labels = [], []\n        for l in lines:\n            label = [float(l[1]), float(l[2]), float(l[3])]\n            files.append(l[0] + '.fused.full.npy.gz')\n            labels.append(label)\n        return files, labels\n\n    def __getitem__(self, item):\n        l = min(self.lengths[item], self.max_length)\n        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n\n    def __len__(self):\n        return len(self.files)\n      \nif __name__ == '__main__':\n    specs = SpectrogramDataset('../input/data/data/multitask_dataset_beat', train=True, max_length=-1)\n    train_loader, val_loader, test_loader = torch_train_val_split(specs, 32 ,32, val_size=.4)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ac33ce9a0ff3d080d09eeaca050b0777e95e8c2d"
      },
      "cell_type": "markdown",
      "source": " ## *Οι συγγραφείς του [7](https://arxiv.org/pdf/1706.05137.pdf) προτείνουν μια καθολική μηχανή εκμάθησης για διάφορα tasks (classification στο ImageNet, translation, WSJ speech recognition etc). Η προτεινόμενη αρχιτεκτονική αποτελείται κυρίως από κάποια modality nets, υπέυθυνα για την εξαγωγή κοινής αναπαράστασης για τα inputs των διάφορων tasks, έναν encoder, έναν mixer και έναν auto-regressive decoder. Τα αποτελέσματα φαίνεται να δείχνουν πως η εκμάθηση πολλών tasks ταυτόχρονα ενισχύει τη δυνατότητα ακρίβειας και γενίκευσης του μοντέλου.*"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3cf5f14082e329dd65cce40cfc90445d11121cad"
      },
      "cell_type": "code",
      "source": "import torch.nn as nn\ntorch.set_default_tensor_type(torch.DoubleTensor)\n\nclass UniversalCNN(nn.Module):\n    def __init__(self, input_dim, output_dim, channels=[12, 24, 48, 96], kernels=[3, 3, 3, 2]):\n        super(UniversalCNN, self).__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.channels = channels\n        \n        self.layer1 = nn.Sequential(nn.Conv2d(in_channels=self.input_dim, out_channels=channels[0], kernel_size=kernels[0]),\n                                    nn.BatchNorm2d(channels[0]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[0]))\n        self.layer2 = nn.Sequential(nn.Conv2d(in_channels=channels[0], out_channels=channels[1], kernel_size=kernels[1]),\n                                    nn.BatchNorm2d(channels[1]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[1]))\n        self.layer3 = nn.Sequential(nn.Conv2d(in_channels=channels[1], out_channels=channels[2], kernel_size=kernels[2]),\n                                    nn.BatchNorm2d(channels[2]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[2]))\n        self.layer4 = nn.Sequential(nn.Conv2d(in_channels=channels[2], out_channels=channels[3], kernel_size=kernels[3]),\n                                    nn.BatchNorm2d(channels[3]),\n                                    nn.ReLU(inplace=True),\n                                    nn.MaxPool2d(kernel_size=kernels[3]))\n        self.linear = nn.Sequential(nn.Linear(channels[3], self.output_dim),\n                                    nn.Sigmoid())\n    \n    def forward(self, x):\n        x = x.view(x.shape[0], self.input_dim, x.shape[2], x.shape[1])\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = x.view(x.shape[0], -1)#, self.channels[3]) #<- Shmantikh (?) allagh\n        x = self.linear(x)\n        \n        return x",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7743048adc5ef3b8c693dc200413945bbfc1a22"
      },
      "cell_type": "code",
      "source": "import scipy.stats as scp\n\ndef evalCNN(model, loss_function, data_loader, weight, axis=[0, 1, 2]):\n    model.eval()\n    with torch.no_grad():\n        valscores = [[], [], []]\n        gold = [[], [], []]\n        metric = [[], [], []]\n        for i in axis:\n            valscores[i] = torch.tensor([])  \n            gold[i] = torch.tensor([])\n        ValLoss = 0\n        for feats, labels, lens in data_loader:\n            scores = model(feats)\n            for i in axis:\n                valscores[i] = torch.cat((valscores[i], scores.view(-1, 3)[:,i]))\n                gold[i] = torch.cat((gold[i], labels[:,i]))\n                ValLoss += weight[i]*loss_function(scores.view(-1, 3)[:,i], labels[:,i])\n        for i in axis:\n            metric[i], _ = scp.spearmanr(valscores[i].detach().numpy(), gold[i])\n\n    return sum(metric)/3, ValLoss\n            \ndef trainCNN(model, loss_function, optimizer, epochs, axis=[0, 1, 2], weight=[1/3, 1/2, 1]):\n    best_metric = 0\n    for epoch in range(epochs):\n        TrainLoss = 0\n        model.train()\n        for feats, labels, lens in (train_loader):\n            # Step 1. Remember that Pytorch accumulates gradients.\n            # We need to clear them out before each instance\n            model.zero_grad()\n            optimizer.zero_grad()\n\n            # Step 3. Run our forward pass.\n            pred_labels = model(feats)\n            #print(pred_labels.shape)\n            \n            # Step 4. Compute the loss, gradients, and update the parameters by\n            #  calling optimizer.step()\n            for i in axis:\n                loss = weight[i]*loss_function(pred_labels.view(-1, 3)[:,i], labels[:,i])\n                TrainLoss += loss\n            loss.backward()\n            optimizer.step()\n        \n        metric, ValLoss = evalCNN(model, loss_function, val_loader, weight, axis)\n        print(\"Epoch: \" + str(epoch) + \" || train loss: \" + str(TrainLoss) + \" & val loss: \" + str(ValLoss))\n        print(\"Metric is: \" + str(metric))\n        if metric > best_metric:\n            best_metric = metric\n            best_model = copy.deepcopy(model)\n            \n    return best_model",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "04aa373414f2793569e2497a71082acae1331961"
      },
      "cell_type": "code",
      "source": "model = UniversalCNN(input_dim=1, output_dim=3, kernels=[(2,4), (1,3), 2, (13,2)])\nloss_function = nn.SmoothL1Loss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\nmodel1 = trainCNN(model, loss_function, optimizer, 10, weight=[1/8, 1/2, 1]) # arbitrarily chosen\n\nm, _ = evalCNN(model1, loss_function, val_loader, [1/8, 1/2, 1])\n\nprint(\"Multitask accuracy (on validation set) is: \" + str(m))\n\nm, _ = evalCNN(model1,loss_function, test_loader, [1/8 , 1/2, 1])\n               \nprint(\"Multitask accuracy (on test set) is: \" + str(m))",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch: 0 || train loss: tensor(1.1357, grad_fn=<AddBackward0>) & val loss: tensor(0.4948)\nMetric is: 0.02690643100801258\nEpoch: 1 || train loss: tensor(0.9430, grad_fn=<AddBackward0>) & val loss: tensor(0.4431)\nMetric is: 0.23781810885598395\nEpoch: 2 || train loss: tensor(0.9481, grad_fn=<AddBackward0>) & val loss: tensor(0.4719)\nMetric is: 0.28059616650054164\nEpoch: 3 || train loss: tensor(1.0143, grad_fn=<AddBackward0>) & val loss: tensor(0.5030)\nMetric is: 0.25847264211340143\nEpoch: 4 || train loss: tensor(0.9758, grad_fn=<AddBackward0>) & val loss: tensor(0.4961)\nMetric is: 0.27100194781714\nEpoch: 5 || train loss: tensor(0.9728, grad_fn=<AddBackward0>) & val loss: tensor(0.4855)\nMetric is: 0.27247804635461415\nEpoch: 6 || train loss: tensor(0.9517, grad_fn=<AddBackward0>) & val loss: tensor(0.4989)\nMetric is: 0.252608104181997\nEpoch: 7 || train loss: tensor(0.9626, grad_fn=<AddBackward0>) & val loss: tensor(0.4945)\nMetric is: 0.27645406076051965\nEpoch: 8 || train loss: tensor(0.9589, grad_fn=<AddBackward0>) & val loss: tensor(0.5066)\nMetric is: 0.23825289002397787\nEpoch: 9 || train loss: tensor(0.9191, grad_fn=<AddBackward0>) & val loss: tensor(0.4936)\nMetric is: 0.243734275599523\nMultitask accuracy (on validation set) is: 0.28059616650054164\nMultitask accuracy (on test set) is: 0.23752111331033987\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e3794cd4b76278f4b1f45c15d35a91230eadf9f8"
      },
      "cell_type": "markdown",
      "source": "## Το μοντέλο μας παράγει χειρότερα scores, απ' τις προηγούμενες μεθόδους που εξετάσαμε, ωστόσο δεν πειραματιστήκαμε ιδιαίτερα με τις υπερπαραμέτρους. Φυσικά, η επίδραση των βαρών με τα οποία πολ/νταί τα επιμέρους losses παίζει σημαντικό ρόλο στην ακρίβεια του μοντέλου μας. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d8bae416d00dd8801c7356dfc7f68ad03b312a1b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}